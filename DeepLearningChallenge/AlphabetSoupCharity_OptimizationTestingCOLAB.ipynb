{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2THBChxYXYxb"
      },
      "outputs": [],
      "source": [
        "# Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "#  Import and read the charity_data.csv.\n",
        "import pandas as pd\n",
        "application_df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")\n",
        "application_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
        "X = application_df.copy().drop(columns=[\"EIN\", \"NAME\"])\n",
        "X.columns"
      ],
      "metadata": {
        "id": "Y_RYzVh0g3i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of unique values in each column.\n",
        "X.nunique()"
      ],
      "metadata": {
        "id": "QcfmO8BKg5-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at APPLICATION_TYPE value counts to identify and replace with \"Other\"\n",
        "application_type_counts = X['APPLICATION_TYPE'].value_counts()\n",
        "application_type_counts"
      ],
      "metadata": {
        "id": "Qdl8MLCog9wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a cutoff value and create a list of application types to be replaced\n",
        "# use the variable name `application_types_to_replace`\n",
        "application_types_to_replace = application_type_counts[application_type_counts < 500].index\n",
        "\n",
        "# Replace in dataframe\n",
        "for app in application_types_to_replace:\n",
        "    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
        "\n",
        "# Check to make sure replacement was successful\n",
        "application_df['APPLICATION_TYPE'].value_counts()"
      ],
      "metadata": {
        "id": "zWdxKN8dg986"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at CLASSIFICATION value counts to identify and replace with \"Other\"\n",
        "classification_counts = X['CLASSIFICATION'].value_counts()\n",
        "classification_counts"
      ],
      "metadata": {
        "id": "8KaHyKGAg-GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You may find it helpful to look at CLASSIFICATION value counts >1\n",
        "classification_counts[classification_counts > 1]"
      ],
      "metadata": {
        "id": "aUHWAtnzg-Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a cutoff value and create a list of classifications to be replaced\n",
        "# use the variable name `classifications_to_replace`\n",
        "classifications_to_replace = classification_counts[classification_counts < 500].index\n",
        "\n",
        "# Replace in dataframe\n",
        "for cls in classifications_to_replace:\n",
        "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
        "\n",
        "# Check to make sure replacement was successful\n",
        "application_df['CLASSIFICATION'].value_counts()"
      ],
      "metadata": {
        "id": "3msdYVhqg-WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "application_df.head()"
      ],
      "metadata": {
        "id": "VA7IIEDohME-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical data to numeric with `pd.get_dummies`\n",
        "X2_df = pd.get_dummies(X)\n",
        "X2_df.head()"
      ],
      "metadata": {
        "id": "-UyYR7mkhOPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our preprocessed data into our features and target arrays\n",
        "X = X2_df.drop(columns=[\"IS_SUCCESSFUL\"]).values\n",
        "y = X2_df[\"IS_SUCCESSFUL\"].values\n",
        "\n",
        "# Split the preprocessed data into a training and testing dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XFBWowdZhObe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a StandardScaler instances\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "8mbFDBz9hOmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile, Train and Evaluate the Model"
      ],
      "metadata": {
        "id": "4VyI0tbvhZVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of input features\n",
        "input_features = X_train.shape[1]  # Number of columns in X_train\n",
        "\n",
        "print(f\"Number of input features: {input_features}\")"
      ],
      "metadata": {
        "id": "OmMjfNg9haWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model B"
      ],
      "metadata": {
        "id": "Vh4vRnMrhpXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
        "def create_model_b():\n",
        "    nn_b = tf.keras.models.Sequential()\n",
        "    nn_b.add(tf.keras.layers.Dense(units=100, activation='relu', input_dim=116))\n",
        "    nn_b.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
        "    nn_b.add(tf.keras.layers.Dense(units=25, activation='relu'))\n",
        "    nn_b.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "    return nn_b\n",
        "\n",
        "# Create the model\n",
        "model_b = create_model_b()\n",
        "\n",
        "# Check the structure of the model\n",
        "model_b.summary()"
      ],
      "metadata": {
        "id": "ULYAdCEqhrDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model B\n",
        "model_b.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "aSLC532Bhso1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model B\n",
        "history_b = model_b.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test))"
      ],
      "metadata": {
        "id": "3akyVVwChuzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model B using the test data\n",
        "model_loss_b, model_accuracy_b = model_b.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "metadata": {
        "id": "m5xg7yB0hwSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export model B to HDF5 file\n",
        "model_b.save(\"AlphabetSoupCharity_OptimizationB.h5\")"
      ],
      "metadata": {
        "id": "LBPhURRChyHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model C"
      ],
      "metadata": {
        "id": "hPFLnly1hzwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
        "def create_model_c():\n",
        "    nn_c = tf.keras.models.Sequential()\n",
        "    nn_c.add(tf.keras.layers.Dense(units=64, activation='relu', input_dim=116, kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
        "    nn_c.add(tf.keras.layers.Dropout(0.2))\n",
        "    nn_c.add(tf.keras.layers.Dense(units=32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
        "    nn_c.add(tf.keras.layers.Dropout(0.2))\n",
        "    nn_c.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "    return nn_c\n",
        "\n",
        "# Create the model\n",
        "model_c = create_model_c()\n",
        "\n",
        "# Check the structure of the model\n",
        "model_c.summary()"
      ],
      "metadata": {
        "id": "w_6wvFs8h0xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model C\n",
        "model_c.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "3lKBtcUVh2zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model C\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history_c = model_c.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test), callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "5m_L68D5h40c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model C using the test data\n",
        "model_loss_c, model_accuracy_c = model_c.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "metadata": {
        "id": "i8PUNgMUh6dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export model C to HDF5 file\n",
        "model_c.save(\"AlphabetSoupCharity_OptimizationC.h5\")"
      ],
      "metadata": {
        "id": "3QlGuxm1h8l-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}